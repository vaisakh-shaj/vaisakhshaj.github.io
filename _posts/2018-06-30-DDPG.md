---
title: 'Deep Deterministic Policy Gradients'
date: 2018-06-30
mathjax: true
image: https://github.com/vaisakh-shaj/vaisakh-shaj.github.io/blob/master/_posts/Images/ddpg.png
permalink: /posts/2018/06/DDPG/
tags:
  - DeepReinforcementLearning
---
#DDPG: Deep Deterministic Policy Gradients

This blogpost will talk about Deep Deterministic Policy Gradients. """


<p align="center">
 Â <img src="https://github.com/vaisakh-shaj/vaisakh-shaj.github.io/blob/master/_posts/Images/ddpg.png?raw=true" alt="Photo" style="width: 450px;"/>
</p>
![](Images/ddpg.PNG)

The popular DQN solves problems with high-dimensional observation spaces, it can only handle
discrete and low-dimensional action spaces. Many tasks of interest, most notably physical control
tasks, have continuous (real valued) and high dimensional action spaces.

To extend DQN to continuous control tasks, we need to perform an extra expensive optimization procedure in the inner loop of training as shown below:



One may use SGD to solve this but it turns out to be quite slow. The other solutions includes sampling actions, CEM, CMA-ES etc , which often works for action dimensions upto 40. Another option is to use [Easily maximizable Q Functions](https://arxiv.org/pdf/1603.00748.pdf), where you gain computational efficiency and simplicity but loose representational power. DDPG is the third approach and most widely used out of its simplicity in implementation, and similarity to well known topics like Q Learning and Policy Gradients.


**DDPG:** can be interpreted in terms of Q Learning and Policy Gradients Literature. In terms of Q Learning, it tends to use a <span style="color:#FF0000">function approximator</span>  for solving the argmax in the Bellman Equation of Q Learning(**approximate Q Learning Method**).

This is achieved by using an actor neural network $\mu_\theta$, and updating its parameter as:

\\[
\begin{aligned}
\theta :=\quad&  \theta\quad + \quad \alpha\times\nabla_\theta Q(s,\mu_\theta(s)) \newline
=\quad & \theta \quad + \quad \alpha\times\nabla_{\mu_\theta} Q(s,\mu_\theta(s)) \times \nabla_{\theta}\mu_\theta(s)  \newline
 =\quad & \theta \quad+\quad\alpha\times\nabla_{a} Q(s,\mu_\theta(s))\times\nabla_{\theta}\mu_\theta(s)
 \end{aligned}
\\]

The equation above has a form of Policy Gradient theorem and its convergence is proven. This is the policy gradient explanation for 2 and hence the name DD-Policy Gradient.

To be continued ..
